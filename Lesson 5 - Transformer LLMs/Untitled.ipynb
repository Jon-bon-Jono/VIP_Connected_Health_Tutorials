{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9411eac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PT, INR, APTT, FIBRINOGEN_LEVEL, CHOLESTEROL_LEVEL, LDL_CHOLESTEROL, VLDL_CHOLESTEROL_LEVEL, VLDL_MEASURED, HDL_CHOLESTEROL, TRIGLYCERIDE_LEVEL, LIPOPROTEIN_ELECTROPHORESIS, APOLIPROTEIN_A1, APOLIPROTEIN_B, APOLIPROTEIN_A1_B_RATIO, APOLIPROTEIN_A, APOLIPROTEIN_B_FAMILIAL, APOLIPROTEIN_E_GRENOLYPE, ANTIBODYTITRE, ANTIBODYTITRESPECIFICITY, PARALLELANTIBODYTITRE, BLOODGROUP, BLOODGROUP_INITIAL, ANTIBODY_SCREEN, SARSCOV2IGG, SARSCOV2SITE, SARSCOV2RNA, SARS_COV_2_RNA_SUPPLEMENTARY_SITE, SARS_COV_2_RNA_SUPPLEMENTARY, SODIUM, POTASSIUM_LEVEL, CHLORIDE_LEVEL, BICARBONATE_LEVEL, UREA_LEVEL, CREATININE, EGFR_CKD_EPI, CALCIUM_LEVEL, ALBUMIN_LEVEL, CORRECTED_CALCIUM_LEVEL, MAGNESIUM_LEVEL, PHOSPHATE_LEVEL, ARTERIAL_BLOOD_PH, ARTERIAL_BLOOD_PCO2, ARTERIAL_BLOOD_PO2, ARTERIAL_BLOOD_HCO3, ARTERIAL_BLOOD_BASE_EXCESS, ARTERIAL_BLOOD_SODIUM, ARTERIAL_BLOOD_POTASSIUM, ARTERIAL_BLOOD_CALCIUMIONISED, ARTERIAL_BLOOD_GLUCOSE, ARTERIAL_BLOOD_LACTATE, ARTERIAL_BLOOD_INSPIRE_OXYGEN, ARTERIAL_BLOOD_OXYGEN_DELIVERY, ARTERIAL_BLOOD_HAEMOGLOBIN, ARTERIAL_BLOOD_REDUCED_HAEMOGLOBIN, ARTERIAL_BLOOD_OXY_HAEMOGLOBIN, ARTERIAL_BLOOD_O2_SATURATION, ARTERIAL_BLOOD_METHAEMOGLOBIN, ARTERIAL_BLOOD_CARBOXYHAEMOGLOBIN, ARTERIAL_BLOOD_BILIRUBIN, ARTERIAL_BLOOD_CREATININE, VENOUS_BLOOD_PH, VENOUS_BLOOD_PO2, VENOUS_BLOOD_PCO2, VENOUS_BLOOD_O2_SATURATION, VENOUS_BLOOD_HCO3, VENOUS_BLOOD_BASE_EXCESS, VENOUS_BLOOD_OXYHAEMOGLOBIN, VENOUS_BLOOD_INSPIRE_OXYGEN, VENOUS_BLOOD_OXYGEN_DELIVERY, VENOUS_BLOOD_HAEMOGLOBIN, VENOUS_BLOOD_REDUCED_HAEMOGLOBIN, VENOUS_BLOOD_METHAEMOGLOBIN, VENOUS_BLOOD_CARBOXYHAEMOGLOBIN, VENOUS_BLOOD_BILIRUBIN, VENOUS_BLOOD_CREATININE, VENOUS_BLOOD_SODIUM, VENOUS_BLOOD_POTASSIUM, VENOUS_BLOOD_CHLORIDE, VENOUS_BLOOD_CALCIUMIONISED, VENOUS_BLOOD_GLUCOSE, VENOUS_BLOOD_LACTATE, VENOUS_BLOOD_PH_POCT, VENOUS_BLOOD_PO2_POCT, VENOUS_BLOOD_PCO2_POCT, VENOUS_BLOOD_O2_SATURATION_POCT, VENOUS_BLOOD_HCO3_POCT, VENOUS_BLOOD_BASE_EXCESS_POCT, VENOUS_BLOOD_OXYHAEMOGLOBIN_POCT, VENOUS_BLOOD_INSPIRE_OXYGEN_POCT, VENOUS_BLOOD_HAEMOGLOBIN_POCT, VENOUS_BLOOD_REDUCED_HAEMOGLOBIN_POCT, VENOUS_BLOOD_METHAEMOGLOBIN_POCT, VENOUS_BLOOD_CARBOXYHAEMOGLOBIN_POCT, VENOUS_BLOOD_CREATININE_POCT, VENOUS_BLOOD_SODIUM_POCT, VENOUS_BLOOD_POTASSIUM_POCT, VENOUS_BLOOD_CHLORIDE_POCT, VENOUS_BLOOD_CALCIUM_IONISED_POCT, VENOUS_BLOOD_GLUCOSE_POCT, VENOUS_BLOOD_LACTATE_POCT, WCC, HB, PLT, HCT, MCV, RCC, MCH, MCHC, RDW_SD, RDW_CV, MPV, NEUTROPHILS, NEUTROPHILS_ABSOLUTE, LYMPHOCYTES, LYMPHOCYTES_ABSOLUTE, MONOCYTES, MONOCYTES_ABSOLUTE, EOSINOPHILS, EOSINOPHILS_ABSOLUTE, BASOPHILS, BASOPHILS_ABSOLUTE, BILIRUBIN, ALP, GGT, AST, ALT, PROTEINTOTALLEVEL, ALBUMIN_LEVEL, GLUCOSELEVEL, HBA1CNGSP, HBA1CFCC, TROPONIN, ANTI_NUCLEAR_ANTIBODIES, NUCLEAR_ANTIBODIES_HOMOGENOUS, NUCLEAR_ANTIBODIES_SPECKLED, CENTROMERE_ANTIBODIES, NUCLEAR_ANTIBODIES_NUCLEOLAR, NUCLEAR_ANTIBODIES_PERIPHERAL, NUCLEAR_ANTIBODIES_CYTOPLASMIC, NUCLEAR_ANTIBODIES_MITOTIC_SPINDLE, NUCLEAR_ANTIBODIES_ATYPICAL_SPECKLED, NUCLEAR_ANTIBODIES_NUCLEAR_MEMBRANCE, ENASCREEN, CREACTIVE_PROTEIN, CREATININE_KINASE_LEVEL, RESPIRATORY_RATE, WEIGHT, PULSE, SYSTOLIC_BP, DIASTOLIC_BP, HEIGHT, BMI, O2_SATURATION, BGL, ECG_RHYTHM, GLASGOW_COMA_SCALE, TEMPERATURE, SEDATION_SCORE, OXYGEN_DELIVERY, OXYGEN_FLOW_RATE, APICAL_HEART_RATE, APICAL_HEART_RATE_REGULAR, PERIPHERAL_PULSE_RATE_REGULAR\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"100\n",
    "\n",
    "PT\n",
    "\n",
    "101\n",
    "\n",
    "INR\n",
    "\n",
    "102\n",
    "\n",
    "APTT\n",
    "\n",
    "103\n",
    "\n",
    "FIBRINOGEN_LEVEL\n",
    "\n",
    "104\n",
    "\n",
    "CHOLESTEROL_LEVEL\n",
    "\n",
    "105\n",
    "\n",
    "LDL_CHOLESTEROL\n",
    "\n",
    "106\n",
    "\n",
    "VLDL_CHOLESTEROL_LEVEL\n",
    "\n",
    "107\n",
    "\n",
    "VLDL_MEASURED\n",
    "\n",
    "108\n",
    "\n",
    "HDL_CHOLESTEROL\n",
    "\n",
    "109\n",
    "\n",
    "TRIGLYCERIDE_LEVEL\n",
    "\n",
    "110\n",
    "\n",
    "LIPOPROTEIN_ELECTROPHORESIS\n",
    "\n",
    "111\n",
    "\n",
    "APOLIPROTEIN_A1\n",
    "\n",
    "112\n",
    "\n",
    "APOLIPROTEIN_B\n",
    "\n",
    "113\n",
    "\n",
    "APOLIPROTEIN_A1_B_RATIO\n",
    "\n",
    "114\n",
    "\n",
    "APOLIPROTEIN_A\n",
    "\n",
    "115\n",
    "\n",
    "APOLIPROTEIN_B_FAMILIAL\n",
    "\n",
    "116\n",
    "\n",
    "APOLIPROTEIN_E_GRENOLYPE\n",
    "\n",
    "117\n",
    "\n",
    "ANTIBODYTITRE\n",
    "\n",
    "118\n",
    "\n",
    "ANTIBODYTITRESPECIFICITY\n",
    "\n",
    "119\n",
    "\n",
    "PARALLELANTIBODYTITRE\n",
    "\n",
    "120\n",
    "\n",
    "BLOODGROUP\n",
    "\n",
    "121\n",
    "\n",
    "BLOODGROUP_INITIAL\n",
    "\n",
    "122\n",
    "\n",
    "ANTIBODY_SCREEN\n",
    "\n",
    "123\n",
    "\n",
    "SARSCOV2IGG\n",
    "\n",
    "124\n",
    "\n",
    "SARSCOV2SITE\n",
    "\n",
    "125\n",
    "\n",
    "SARSCOV2RNA\n",
    "\n",
    "126\n",
    "\n",
    "SARS_COV_2_RNA_SUPPLEMENTARY_SITE\n",
    "\n",
    "127\n",
    "\n",
    "SARS_COV_2_RNA_SUPPLEMENTARY\n",
    "\n",
    "128\n",
    "\n",
    "SODIUM\n",
    "\n",
    "129\n",
    "\n",
    "POTASSIUM_LEVEL\n",
    "\n",
    "130\n",
    "\n",
    "CHLORIDE_LEVEL\n",
    "\n",
    "131\n",
    "\n",
    "BICARBONATE_LEVEL\n",
    "\n",
    "132\n",
    "\n",
    "UREA_LEVEL\n",
    "\n",
    "133\n",
    "\n",
    "CREATININE\n",
    "\n",
    "134\n",
    "\n",
    "EGFR_CKD_EPI\n",
    "\n",
    "135\n",
    "\n",
    "CALCIUM_LEVEL\n",
    "\n",
    "136\n",
    "\n",
    "ALBUMIN_LEVEL\n",
    "\n",
    "137\n",
    "\n",
    "CORRECTED_CALCIUM_LEVEL\n",
    "\n",
    "138\n",
    "\n",
    "MAGNESIUM_LEVEL\n",
    "\n",
    "139\n",
    "\n",
    "PHOSPHATE_LEVEL\n",
    "\n",
    "140\n",
    "\n",
    "ARTERIAL_BLOOD_PH\n",
    "\n",
    "141\n",
    "\n",
    "ARTERIAL_BLOOD_PCO2\n",
    "\n",
    "142\n",
    "\n",
    "ARTERIAL_BLOOD_PO2\n",
    "\n",
    "143\n",
    "\n",
    "ARTERIAL_BLOOD_HCO3\n",
    "\n",
    "144\n",
    "\n",
    "ARTERIAL_BLOOD_BASE_EXCESS\n",
    "\n",
    "145\n",
    "\n",
    "ARTERIAL_BLOOD_SODIUM\n",
    "\n",
    "146\n",
    "\n",
    "ARTERIAL_BLOOD_POTASSIUM\n",
    "\n",
    "147\n",
    "\n",
    "ARTERIAL_BLOOD_CALCIUMIONISED\n",
    "\n",
    "148\n",
    "\n",
    "ARTERIAL_BLOOD_GLUCOSE\n",
    "\n",
    "149\n",
    "\n",
    "ARTERIAL_BLOOD_LACTATE\n",
    "\n",
    "150\n",
    "\n",
    "ARTERIAL_BLOOD_INSPIRE_OXYGEN\n",
    "\n",
    "151\n",
    "\n",
    "ARTERIAL_BLOOD_OXYGEN_DELIVERY\n",
    "\n",
    "152\n",
    "\n",
    "ARTERIAL_BLOOD_HAEMOGLOBIN\n",
    "\n",
    "153\n",
    "\n",
    "ARTERIAL_BLOOD_REDUCED_HAEMOGLOBIN\n",
    "\n",
    "154\n",
    "\n",
    "ARTERIAL_BLOOD_OXY_HAEMOGLOBIN\n",
    "\n",
    "155\n",
    "\n",
    "ARTERIAL_BLOOD_O2_SATURATION\n",
    "\n",
    "156\n",
    "\n",
    "ARTERIAL_BLOOD_METHAEMOGLOBIN\n",
    "\n",
    "157\n",
    "\n",
    "ARTERIAL_BLOOD_CARBOXYHAEMOGLOBIN\n",
    "\n",
    "158\n",
    "\n",
    "ARTERIAL_BLOOD_BILIRUBIN\n",
    "\n",
    "159\n",
    "\n",
    "ARTERIAL_BLOOD_CREATININE\n",
    "\n",
    "160\n",
    "\n",
    "VENOUS_BLOOD_PH\n",
    "\n",
    "161\n",
    "\n",
    "VENOUS_BLOOD_PO2\n",
    "\n",
    "162\n",
    "\n",
    "VENOUS_BLOOD_PCO2\n",
    "\n",
    "163\n",
    "\n",
    "VENOUS_BLOOD_O2_SATURATION\n",
    "\n",
    "164\n",
    "\n",
    "VENOUS_BLOOD_HCO3\n",
    "\n",
    "165\n",
    "\n",
    "VENOUS_BLOOD_BASE_EXCESS\n",
    "\n",
    "166\n",
    "\n",
    "VENOUS_BLOOD_OXYHAEMOGLOBIN\n",
    "\n",
    "167\n",
    "\n",
    "VENOUS_BLOOD_INSPIRE_OXYGEN\n",
    "\n",
    "168\n",
    "\n",
    "VENOUS_BLOOD_OXYGEN_DELIVERY\n",
    "\n",
    "169\n",
    "\n",
    "VENOUS_BLOOD_HAEMOGLOBIN\n",
    "\n",
    "170\n",
    "\n",
    "VENOUS_BLOOD_REDUCED_HAEMOGLOBIN\n",
    "\n",
    "171\n",
    "\n",
    "VENOUS_BLOOD_METHAEMOGLOBIN\n",
    "\n",
    "172\n",
    "\n",
    "VENOUS_BLOOD_CARBOXYHAEMOGLOBIN\n",
    "\n",
    "173\n",
    "\n",
    "VENOUS_BLOOD_BILIRUBIN\n",
    "\n",
    "174\n",
    "\n",
    "VENOUS_BLOOD_CREATININE\n",
    "\n",
    "175\n",
    "\n",
    "VENOUS_BLOOD_SODIUM\n",
    "\n",
    "176\n",
    "\n",
    "VENOUS_BLOOD_POTASSIUM\n",
    "\n",
    "177\n",
    "\n",
    "VENOUS_BLOOD_CHLORIDE\n",
    "\n",
    "178\n",
    "\n",
    "VENOUS_BLOOD_CALCIUMIONISED\n",
    "\n",
    "179\n",
    "\n",
    "VENOUS_BLOOD_GLUCOSE\n",
    "\n",
    "180\n",
    "\n",
    "VENOUS_BLOOD_LACTATE\n",
    "\n",
    "181\n",
    "\n",
    "VENOUS_BLOOD_PH_POCT\n",
    "\n",
    "182\n",
    "\n",
    "VENOUS_BLOOD_PO2_POCT\n",
    "\n",
    "183\n",
    "\n",
    "VENOUS_BLOOD_PCO2_POCT\n",
    "\n",
    "184\n",
    "\n",
    "VENOUS_BLOOD_O2_SATURATION_POCT\n",
    "\n",
    "185\n",
    "\n",
    "VENOUS_BLOOD_HCO3_POCT\n",
    "\n",
    "186\n",
    "\n",
    "VENOUS_BLOOD_BASE_EXCESS_POCT\n",
    "\n",
    "187\n",
    "\n",
    "VENOUS_BLOOD_OXYHAEMOGLOBIN_POCT\n",
    "\n",
    "188\n",
    "\n",
    "VENOUS_BLOOD_INSPIRE_OXYGEN_POCT\n",
    "\n",
    "189\n",
    "\n",
    "VENOUS_BLOOD_HAEMOGLOBIN_POCT\n",
    "\n",
    "190\n",
    "\n",
    "VENOUS_BLOOD_REDUCED_HAEMOGLOBIN_POCT\n",
    "\n",
    "191\n",
    "\n",
    "VENOUS_BLOOD_METHAEMOGLOBIN_POCT\n",
    "\n",
    "192\n",
    "\n",
    "VENOUS_BLOOD_CARBOXYHAEMOGLOBIN_POCT\n",
    "\n",
    "193\n",
    "\n",
    "VENOUS_BLOOD_CREATININE_POCT\n",
    "\n",
    "194\n",
    "\n",
    "VENOUS_BLOOD_SODIUM_POCT\n",
    "\n",
    "195\n",
    "\n",
    "VENOUS_BLOOD_POTASSIUM_POCT\n",
    "\n",
    "196\n",
    "\n",
    "VENOUS_BLOOD_CHLORIDE_POCT\n",
    "\n",
    "197\n",
    "\n",
    "VENOUS_BLOOD_CALCIUM_IONISED_POCT\n",
    "\n",
    "198\n",
    "\n",
    "VENOUS_BLOOD_GLUCOSE_POCT\n",
    "\n",
    "199\n",
    "\n",
    "VENOUS_BLOOD_LACTATE_POCT\n",
    "\n",
    "200\n",
    "\n",
    "WCC\n",
    "\n",
    "201\n",
    "\n",
    "HB\n",
    "\n",
    "202\n",
    "\n",
    "PLT\n",
    "\n",
    "203\n",
    "\n",
    "HCT\n",
    "\n",
    "204\n",
    "\n",
    "MCV\n",
    "\n",
    "205\n",
    "\n",
    "RCC\n",
    "\n",
    "206\n",
    "\n",
    "MCH\n",
    "\n",
    "207\n",
    "\n",
    "MCHC\n",
    "\n",
    "208\n",
    "\n",
    "RDW_SD\n",
    "\n",
    "209\n",
    "\n",
    "RDW_CV\n",
    "\n",
    "210\n",
    "\n",
    "MPV\n",
    "\n",
    "211\n",
    "\n",
    "NEUTROPHILS\n",
    "\n",
    "212\n",
    "\n",
    "NEUTROPHILS_ABSOLUTE\n",
    "\n",
    "213\n",
    "\n",
    "LYMPHOCYTES\n",
    "\n",
    "214\n",
    "\n",
    "LYMPHOCYTES_ABSOLUTE\n",
    "\n",
    "215\n",
    "\n",
    "MONOCYTES\n",
    "\n",
    "216\n",
    "\n",
    "MONOCYTES_ABSOLUTE\n",
    "\n",
    "217\n",
    "\n",
    "EOSINOPHILS\n",
    "\n",
    "218\n",
    "\n",
    "EOSINOPHILS_ABSOLUTE\n",
    "\n",
    "219\n",
    "\n",
    "BASOPHILS\n",
    "\n",
    "220\n",
    "\n",
    "BASOPHILS_ABSOLUTE\n",
    "\n",
    "221\n",
    "\n",
    "BILIRUBIN\n",
    "\n",
    "222\n",
    "\n",
    "ALP\n",
    "\n",
    "223\n",
    "\n",
    "GGT\n",
    "\n",
    "224\n",
    "\n",
    "AST\n",
    "\n",
    "225\n",
    "\n",
    "ALT\n",
    "\n",
    "226\n",
    "\n",
    "PROTEINTOTALLEVEL\n",
    "\n",
    "227\n",
    "\n",
    "ALBUMIN_LEVEL\n",
    "\n",
    "228\n",
    "\n",
    "GLUCOSELEVEL\n",
    "\n",
    "229\n",
    "\n",
    "HBA1CNGSP\n",
    "\n",
    "230\n",
    "\n",
    "HBA1CFCC\n",
    "\n",
    "231\n",
    "\n",
    "TROPONIN\n",
    "\n",
    "232\n",
    "\n",
    "ANTI_NUCLEAR_ANTIBODIES\n",
    "\n",
    "233\n",
    "\n",
    "NUCLEAR_ANTIBODIES_HOMOGENOUS\n",
    "\n",
    "234\n",
    "\n",
    "NUCLEAR_ANTIBODIES_SPECKLED\n",
    "\n",
    "235\n",
    "\n",
    "CENTROMERE_ANTIBODIES\n",
    "\n",
    "236\n",
    "\n",
    "NUCLEAR_ANTIBODIES_NUCLEOLAR\n",
    "\n",
    "237\n",
    "\n",
    "NUCLEAR_ANTIBODIES_PERIPHERAL\n",
    "\n",
    "238\n",
    "\n",
    "NUCLEAR_ANTIBODIES_CYTOPLASMIC\n",
    "\n",
    "239\n",
    "\n",
    "NUCLEAR_ANTIBODIES_MITOTIC_SPINDLE\n",
    "\n",
    "240\n",
    "\n",
    "NUCLEAR_ANTIBODIES_ATYPICAL_SPECKLED\n",
    "\n",
    "241\n",
    "\n",
    "NUCLEAR_ANTIBODIES_NUCLEAR_MEMBRANCE\n",
    "\n",
    "242\n",
    "\n",
    "ENASCREEN\n",
    "\n",
    "243\n",
    "\n",
    "CREACTIVE_PROTEIN\n",
    "\n",
    "244\n",
    "\n",
    "CREATININE_KINASE_LEVEL\n",
    "\n",
    "245\n",
    "\n",
    "RESPIRATORY_RATE\n",
    "\n",
    "246\n",
    "\n",
    "WEIGHT\n",
    "\n",
    "247\n",
    "\n",
    "PULSE\n",
    "\n",
    "248\n",
    "\n",
    "SYSTOLIC_BP\n",
    "\n",
    "249\n",
    "\n",
    "DIASTOLIC_BP\n",
    "\n",
    "250\n",
    "\n",
    "HEIGHT\n",
    "\n",
    "251\n",
    "\n",
    "BMI\n",
    "\n",
    "252\n",
    "\n",
    "O2_SATURATION\n",
    "\n",
    "253\n",
    "\n",
    "BGL\n",
    "\n",
    "254\n",
    "\n",
    "ECG_RHYTHM\n",
    "\n",
    "255\n",
    "\n",
    "GLASGOW_COMA_SCALE\n",
    "\n",
    "256\n",
    "\n",
    "TEMPERATURE\n",
    "\n",
    "257\n",
    "\n",
    "SEDATION_SCORE\n",
    "\n",
    "258\n",
    "\n",
    "OXYGEN_DELIVERY\n",
    "\n",
    "259\n",
    "\n",
    "OXYGEN_FLOW_RATE\n",
    "\n",
    "260\n",
    "\n",
    "APICAL_HEART_RATE\n",
    "\n",
    "261\n",
    "\n",
    "APICAL_HEART_RATE_REGULAR\n",
    "\n",
    "262\n",
    "\n",
    "PERIPHERAL_PULSE_RATE_REGULAR\"\"\"\n",
    "import re\n",
    "new_text = text.split(\"\\n\")\n",
    "new_text = [t for t in new_text if t and not bool(re.match(r'^\\d+$', t))]\n",
    "tex = \", \".join(new_text)\n",
    "print(tex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f754731",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\z5162987\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "C:\\Users\\z5162987\\anaconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:260: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (2269 > 512). Running this sequence through the model will result in indexing errors\n",
      "Processing:   0%|                                                                           | 0/217 [00:00<?, ?batch/s]C:\\Users\\z5162987\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5476: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = scaled_dot_product_attention(q, k, v, attn_mask, dropout_p, is_causal)\n",
      "C:\\Users\\z5162987\\anaconda3\\Lib\\site-packages\\torch\\nn\\functional.py:5109: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
      "  warnings.warn(\n",
      "Processing: 100%|█████████████████████████████████████████████████████████████████| 217/217 [00:51<00:00,  4.19batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.7121620806473766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████████████████████████████████████████| 217/217 [00:52<00:00,  4.10batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 0.000989901172029378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████████████████████████████████████████| 217/217 [00:53<00:00,  4.06batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 0.00047481878932076186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████████████████████████████████████████| 217/217 [00:52<00:00,  4.13batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 0.0002954632056177452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|█████████████████████████████████████████████████████████████████| 217/217 [00:52<00:00,  4.15batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 0.00020527345988596897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nweights = torch.load(f\"self_attention_llm_{MODEL_DIM}_{NUM_HEADS}_{max_length}.pth\")\\nmodel = SelfAttentionLLM().to(DEVICE)\\nmodel.load_state_dict(weights)\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#PyTorch based Attention and Transformer LLM\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from torchtext.datasets import Multi30k\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "# Configurations\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "MODEL_DIM = 768  # Dimensionality of token embedding space\n",
    "NUM_HEADS = 12   # Number of attention heads\n",
    "NUM_LAYERS = 2\n",
    "max_length = 100\n",
    "PAD_IDX=0\n",
    "# Load BERT tokenizer and model for positional encoding\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased',max_length=max_length) #bert does like a n-ary tokenization scheme\n",
    "bert_model = BertModel.from_pretrained('bert-base-uncased',max_length=max_length).to(DEVICE) #import bert model to use the pre-trained positional encoder\n",
    "\n",
    "def load_olympic_train_data():\n",
    "    with open('Australian_Olympics_21ct.txt', 'r', encoding=\"utf8\") as file:\n",
    "        lines = file.readlines()\n",
    "    lines = lines[:100]\n",
    "    lines = [line.strip() for line in lines if line.strip()]\n",
    "    train_data = \" \".join(lines)\n",
    "    return train_data\n",
    "\n",
    "def load_translation_train_data():\n",
    "    train_data, _, _ = Multi30k(split=('train', 'valid', 'test'), language_pair=('en','de'))\n",
    "    train_data = list(train_data)[:100]\n",
    "    train_data = [d[0] for d in train_data]\n",
    "    train_data = \" \".join(train_data)\n",
    "    return train_data\n",
    "\n",
    "train_data = load_olympic_train_data()\n",
    "\n",
    "#train_data = text\n",
    "class TextDataset(Dataset):\n",
    "    #self.inputs is a list of lists containing max_length token ids from a sliding window over our text dataset\n",
    "    #self.targets is a list of lists containing the max_length token ids that come after corresponding inputs\n",
    "    def __init__(self, text, max_length, tokenizer):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.inputs = []\n",
    "        self.targets = []\n",
    "        self.max_length = max_length\n",
    "        tokenized_text = self.tokenizer.encode(text, add_special_tokens=True)\n",
    "        for i in range(0, len(tokenized_text) - max_length, 1):\n",
    "            input_ids = tokenized_text[i:i+max_length]\n",
    "            target_ids = tokenized_text[i+1:i+max_length+1]\n",
    "            # Add padding if necessary\n",
    "            input_ids += [tokenizer.pad_token_id] * (max_length - len(input_ids))\n",
    "            target_ids += [tokenizer.pad_token_id] * (max_length - len(target_ids))\n",
    "            self.inputs.append(input_ids)\n",
    "            self.targets.append(target_ids)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.inputs[idx]), torch.tensor(self.targets[idx])\n",
    "\n",
    "class SelfAttentionLLM(nn.Module):\n",
    "    def __init__(self, bert_embeddings=True):\n",
    "        super().__init__()\n",
    "        if bert_embeddings:\n",
    "            self.embedding = bert_model.embeddings.word_embeddings\n",
    "        else:\n",
    "            self.embedding = nn.Embedding(tokenizer.vocab_size, MODEL_DIM)\n",
    "        self.positional_encodings = bert_model.embeddings.position_embeddings\n",
    "        # Freeze the BERT parameters if you don't want to fine-tune the model\n",
    "        for param in bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.multi_head_attention = nn.MultiheadAttention(MODEL_DIM, NUM_HEADS)\n",
    "        self.linear = nn.Linear(MODEL_DIM, tokenizer.vocab_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: torch.Size([batch_size, max_length])\n",
    "        \n",
    "        positions = torch.arange(0, x.size(1), device=x.device).unsqueeze(0).expand(x.size(0), x.size(1))\n",
    "        #torch.arange(0, x.size(1), device=x.device) generates a 1D tensor with values ranging from 0 to x.size(1) - 1 (i.e., 0 to max_length-1).\n",
    "        #.unsqueeze(0) adds a batch dimension, converting the tensor shape from (max_length) to (1, max_length)\n",
    "        #.expand(x.size(0), x.size(1)) replicates this tensor across the batch dimension, resulting in a shape of (batch_size, max_length).\n",
    "        # positions: torch.Size([batch_size, max_length])\n",
    "        \n",
    "        x = self.embedding(x) + self.positional_encodings(positions)\n",
    "        #self.embedding(x): torch.Size([batch_size, max_length, MODEL_DIM])\n",
    "        #self.positional_encodings(positions): torch.Size([batch_size, max_length, MODEL_DIM])\n",
    "        \n",
    "        attn_output, _ = self.multi_head_attention(x, x, x)\n",
    "        #attn_output: torch.Size([batch_size, max_length, MODEL_DIM])\n",
    "        \n",
    "        logits = self.linear(attn_output)\n",
    "        #logits: torch.Size([batch_size, max_length, vocab_size])\n",
    "        return F.log_softmax(logits, dim=-1)\n",
    "\n",
    "class TransformerLLM(nn.Module):\n",
    "    def __init__(self, dropout=0.1, bert_embeddings=True):\n",
    "        super(TransformerLLM, self).__init__()\n",
    "        \n",
    "        if bert_embeddings:\n",
    "            self.src_embedding = bert_model.embeddings.word_embeddings\n",
    "            self.tgt_embedding = bert_model.embeddings.word_embeddings\n",
    "        else:\n",
    "            self.src_embedding = nn.Embedding(tokenizer.vocab_size, MODEL_DIM)\n",
    "            self.tgt_embedding = nn.Embedding(tokenizer.vocab_size, MODEL_DIM)\n",
    "        self.positional_encodings = bert_model.embeddings.position_embeddings\n",
    "        # Freeze the BERT parameters if you don't want to fine-tune the model\n",
    "        for param in bert_model.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model=MODEL_DIM,\n",
    "            nhead=NUM_HEADS,\n",
    "            num_encoder_layers=NUM_LAYERS,\n",
    "            num_decoder_layers=NUM_LAYERS,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        self.output_linear = nn.Linear(MODEL_DIM, tokenizer.vocab_size)\n",
    "\n",
    "        \n",
    "    def forward(self, src, tgt, src_mask=None, tgt_mask=None, memory_mask=None, inference=False):\n",
    "        #src: torch.Size([batch_size, max_length])\n",
    "        #During training:\n",
    "        #tgt: torch.Size([batch_size, max_length])\n",
    "        #During inference\n",
    "        #tgt: torch.Size([batch_size, i+1]) where i is the number of decoding loop iterations\n",
    "        #let seq_length = max_length if training else i+1 \n",
    "        \n",
    "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = self.create_mask(src, tgt)\n",
    "        #src_mask: torch.Size([100, 100])\n",
    "        #tgt_mask: torch.Size([100, 100])\n",
    "        #src_padding_mask: torch.Size([100, batch_size])\n",
    "        #tgt_padding_mask: torch.Size([100, batch_size])\n",
    "        \n",
    "        src_padding_mask = src_padding_mask.transpose(1,0)\n",
    "        tgt_padding_mask = tgt_padding_mask.transpose(1,0)\n",
    "        #src_padding_mask2: torch.Size([10, 100])\n",
    "        #tgt_padding_mask2: torch.Size([10, 100])\n",
    "\n",
    "        src_positions = torch.arange(0, src.size(1), device=src.device).unsqueeze(0).expand(src.size(0), src.size(1))\n",
    "        #torch.arange(0, src.size(1), device=src.device) generates a 1D tensor with values ranging from 0 to src.size(1) - 1 (i.e., 0 to max_length-1).\n",
    "        #.unsqueeze(0) adds a batch dimension, converting the tensor shape from (max_length) to (1, max_length)\n",
    "        #.expand(src.size(0), src.size(1)) replicates this tensor across the batch dimension, resulting in a shape of (batch_size, max_length).\n",
    "        \n",
    "        tgt_positions = torch.arange(0, tgt.size(1), device=tgt.device).unsqueeze(0).expand(tgt.size(0), tgt.size(1))\n",
    "        \n",
    "        # src_positions: torch.Size([batch_size, max_length])\n",
    "        # tgt_positions: torch.Size([batch_size, seq_length])\n",
    "        \n",
    "        src = self.src_embedding(src) + self.positional_encodings(src_positions)\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encodings(tgt_positions)\n",
    "        #src: torch.Size([10, 100, 768])\n",
    "        #tgt: torch.Size([10, 100, 768])\n",
    "        if inference: \n",
    "            decoder_output = self.transformer(src, tgt, src_mask, None, None, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        else:\n",
    "            decoder_output = self.transformer(src, tgt, src_mask, tgt_mask, None, src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
    "        #decoder_output.shape: torch.Size([10, 100, 768])\n",
    "        #output = self.transformer(src, tgt, src_mask, tgt_mask, None,None,None,None)\n",
    "        output = self.output_linear(decoder_output)\n",
    "        #output.shape: torch.Size([10, 100, 30522])\n",
    "        return F.log_softmax(output, dim=-1)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        #sz : seq_length\n",
    "        mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        #mask: (seq_length, seq_length)\n",
    "        #mask is the lower triangular matrix where the upper terms past the diagonal are set to −∞\n",
    "        return mask\n",
    "\n",
    "    def create_mask(self, src, tgt):\n",
    "        src_seq_len = src.shape[1] #max_length\n",
    "        tgt_seq_len = tgt.shape[1] #seq_length\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(tgt_seq_len) #(seq_length, seq_length)\n",
    "        src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
    "\n",
    "        src_padding_mask = (src == PAD_IDX).transpose(0, 1)\n",
    "        tgt_padding_mask = (tgt == PAD_IDX).transpose(0, 1)\n",
    "        return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask\n",
    "    \n",
    "    \n",
    "    def create_mask_old(self, src, tgt):\n",
    "        # Create a mask to hide padding for source sequences\n",
    "        src_pad_mask = (src == 0)  # Assuming that '0' is the padding token\n",
    "        src_mask = src_pad_mask.transpose(0, 1)  # Shape: [batch_size, src_seq_len]\n",
    "\n",
    "        # Create a mask to hide future words for target sequences during training\n",
    "        tgt_seq_len = tgt.size(1)\n",
    "        tgt_mask = torch.triu(torch.ones((tgt_seq_len, tgt_seq_len), device=tgt.device), diagonal=1).bool()\n",
    "        \n",
    "        # Memory mask to prevent decoder from attending to future positions\n",
    "        memory_mask = tgt_mask  # Using the same mask for simplicity\n",
    "\n",
    "        return src_mask, tgt_mask, memory_mask\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        src_positions = torch.arange(0, src.size(1), device=src.device).unsqueeze(0).expand(src.size(0), src.size(1))\n",
    "        src = self.src_embedding(src) + self.positional_encodings(src_positions)\n",
    "        return self.transformer.encoder(src, src_mask)\n",
    "\n",
    "    def decode(self, tgt, memory, tgt_mask):\n",
    "        tgt_positions = torch.arange(0, tgt.size(1), device=tgt.device).unsqueeze(0).expand(tgt.size(0), tgt.size(1))\n",
    "        print(f\"tgt_pos: {tgt_positions.shape}\")\n",
    "        tgt = self.tgt_embedding(tgt) + self.positional_encodings(tgt_positions)\n",
    "        print(f\"tgt: {tgt.shape}\")\n",
    "        print(f\"memory: {memory.shape}\")\n",
    "        print(f\"tgt_mask: {tgt_mask.shape}\")\n",
    "        decoded = self.transformer.decoder(tgt, memory, None)\n",
    "        output = self.output_linear(decoded)\n",
    "        return F.log_softmax(output, dim=-1)\n",
    "\n",
    "#when training -> no padding required\n",
    "#during inference we want to build a tensor same size as src with all padding\n",
    "def train(model, data_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    #for inputs, targets in data_loader:\n",
    "    for inputs, targets in tqdm(data_loader, desc=\"Processing\", unit=\"batch\"):\n",
    "        inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        # inputs and targets: [seq_length, batch_size]\n",
    "        if isinstance(model, TransformerLLM):\n",
    "            outputs = model(inputs, targets)  \n",
    "        else:\n",
    "            outputs = model(inputs) # [seq_length, batch_size, vocab_size]\n",
    "        #x=1/0\n",
    "        #first input is vector of logits (len is vocab size)\n",
    "        #2nd input is just the token (class) integer\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    return total_loss / len(data_loader), model\n",
    "\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in data_loader:\n",
    "            inputs, targets = inputs.to(DEVICE), targets.to(DEVICE)\n",
    "            if isinstance(model, TransformerLLM):\n",
    "                outputs = model(inputs, targets)\n",
    "            else:\n",
    "                outputs = model(inputs)\n",
    "            loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1)) \n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(data_loader)\n",
    "\n",
    "# Example Usage\n",
    "texts = train_data\n",
    "dataset = TextDataset(texts, max_length, tokenizer)\n",
    "# print(dataset.tokenizer.decode(dataset[0][0]))\n",
    "# print(\"\\n\")\n",
    "# print(dataset.tokenizer.decode(dataset[1][0]))\n",
    "# print(\"\\n\")\n",
    "# print(dataset.tokenizer.decode(dataset[2][0]))\n",
    "\n",
    "data_loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "#model = SelfAttentionLLM().to(DEVICE)\n",
    "#outputs: torch.Size([10, 100, 30522])\n",
    "model = TransformerLLM().to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "\n",
    "for epoch in range(5):\n",
    "    train_loss, model = train(model, data_loader, optimizer, criterion)\n",
    "    print(f'Epoch {epoch+1}, Train Loss: {train_loss}')\n",
    "torch.save(model.state_dict(), f\"attention_llm_{MODEL_DIM}_{NUM_HEADS}_{max_length}.pth\")\n",
    "\n",
    "\n",
    "'''\n",
    "weights = torch.load(f\"self_attention_llm_{MODEL_DIM}_{NUM_HEADS}_{max_length}.pth\")\n",
    "model = SelfAttentionLLM().to(DEVICE)\n",
    "model.load_state_dict(weights)\n",
    "'''\n",
    "\n",
    "# Evaluate the model\n",
    "#test_loss = evaluate(model, data_loader)\n",
    "#print(f'Test Loss: {test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2659dd3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.vocab['the']\n",
    "#tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a02ea083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = torch.load(f\"self_attention_llm_{MODEL_DIM}_{NUM_HEADS}_{max_length}.pth\")\n",
    "model1 = SelfAttentionLLM().to(DEVICE)\n",
    "model1.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4a428ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src_tensor: torch.Size([1, 5])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt: torch.Size([1, 1])\n",
      "tgt_pos: torch.Size([1, 1])\n",
      "tgt: torch.Size([1, 1, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([1, 1])\n",
      "`output: torch.Size([1, 1, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 2])\n",
      "tgt_pos: torch.Size([1, 2])\n",
      "tgt: torch.Size([1, 2, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([2, 2])\n",
      "`output: torch.Size([1, 2, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 3])\n",
      "tgt_pos: torch.Size([1, 3])\n",
      "tgt: torch.Size([1, 3, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([3, 3])\n",
      "`output: torch.Size([1, 3, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 4])\n",
      "tgt_pos: torch.Size([1, 4])\n",
      "tgt: torch.Size([1, 4, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([4, 4])\n",
      "`output: torch.Size([1, 4, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 5])\n",
      "tgt_pos: torch.Size([1, 5])\n",
      "tgt: torch.Size([1, 5, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([5, 5])\n",
      "`output: torch.Size([1, 5, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 6])\n",
      "tgt_pos: torch.Size([1, 6])\n",
      "tgt: torch.Size([1, 6, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([6, 6])\n",
      "`output: torch.Size([1, 6, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 7])\n",
      "tgt_pos: torch.Size([1, 7])\n",
      "tgt: torch.Size([1, 7, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([7, 7])\n",
      "`output: torch.Size([1, 7, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 8])\n",
      "tgt_pos: torch.Size([1, 8])\n",
      "tgt: torch.Size([1, 8, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([8, 8])\n",
      "`output: torch.Size([1, 8, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 9])\n",
      "tgt_pos: torch.Size([1, 9])\n",
      "tgt: torch.Size([1, 9, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([9, 9])\n",
      "`output: torch.Size([1, 9, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 10])\n",
      "tgt_pos: torch.Size([1, 10])\n",
      "tgt: torch.Size([1, 10, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([10, 10])\n",
      "`output: torch.Size([1, 10, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 11])\n",
      "tgt_pos: torch.Size([1, 11])\n",
      "tgt: torch.Size([1, 11, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([11, 11])\n",
      "`output: torch.Size([1, 11, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 12])\n",
      "tgt_pos: torch.Size([1, 12])\n",
      "tgt: torch.Size([1, 12, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([12, 12])\n",
      "`output: torch.Size([1, 12, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 13])\n",
      "tgt_pos: torch.Size([1, 13])\n",
      "tgt: torch.Size([1, 13, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([13, 13])\n",
      "`output: torch.Size([1, 13, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 14])\n",
      "tgt_pos: torch.Size([1, 14])\n",
      "tgt: torch.Size([1, 14, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([14, 14])\n",
      "`output: torch.Size([1, 14, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 15])\n",
      "tgt_pos: torch.Size([1, 15])\n",
      "tgt: torch.Size([1, 15, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([15, 15])\n",
      "`output: torch.Size([1, 15, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 16])\n",
      "tgt_pos: torch.Size([1, 16])\n",
      "tgt: torch.Size([1, 16, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([16, 16])\n",
      "`output: torch.Size([1, 16, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 17])\n",
      "tgt_pos: torch.Size([1, 17])\n",
      "tgt: torch.Size([1, 17, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([17, 17])\n",
      "`output: torch.Size([1, 17, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 18])\n",
      "tgt_pos: torch.Size([1, 18])\n",
      "tgt: torch.Size([1, 18, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([18, 18])\n",
      "`output: torch.Size([1, 18, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 19])\n",
      "tgt_pos: torch.Size([1, 19])\n",
      "tgt: torch.Size([1, 19, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([19, 19])\n",
      "`output: torch.Size([1, 19, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 20])\n",
      "tgt_pos: torch.Size([1, 20])\n",
      "tgt: torch.Size([1, 20, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([20, 20])\n",
      "`output: torch.Size([1, 20, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 21])\n",
      "tgt_pos: torch.Size([1, 21])\n",
      "tgt: torch.Size([1, 21, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([21, 21])\n",
      "`output: torch.Size([1, 21, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 22])\n",
      "tgt_pos: torch.Size([1, 22])\n",
      "tgt: torch.Size([1, 22, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([22, 22])\n",
      "`output: torch.Size([1, 22, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 23])\n",
      "tgt_pos: torch.Size([1, 23])\n",
      "tgt: torch.Size([1, 23, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([23, 23])\n",
      "`output: torch.Size([1, 23, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 24])\n",
      "tgt_pos: torch.Size([1, 24])\n",
      "tgt: torch.Size([1, 24, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([24, 24])\n",
      "`output: torch.Size([1, 24, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 25])\n",
      "tgt_pos: torch.Size([1, 25])\n",
      "tgt: torch.Size([1, 25, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([25, 25])\n",
      "`output: torch.Size([1, 25, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 26])\n",
      "tgt_pos: torch.Size([1, 26])\n",
      "tgt: torch.Size([1, 26, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([26, 26])\n",
      "`output: torch.Size([1, 26, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 27])\n",
      "tgt_pos: torch.Size([1, 27])\n",
      "tgt: torch.Size([1, 27, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([27, 27])\n",
      "`output: torch.Size([1, 27, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 28])\n",
      "tgt_pos: torch.Size([1, 28])\n",
      "tgt: torch.Size([1, 28, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([28, 28])\n",
      "`output: torch.Size([1, 28, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 29])\n",
      "tgt_pos: torch.Size([1, 29])\n",
      "tgt: torch.Size([1, 29, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([29, 29])\n",
      "`output: torch.Size([1, 29, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 30])\n",
      "tgt_pos: torch.Size([1, 30])\n",
      "tgt: torch.Size([1, 30, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([30, 30])\n",
      "`output: torch.Size([1, 30, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 31])\n",
      "tgt_pos: torch.Size([1, 31])\n",
      "tgt: torch.Size([1, 31, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([31, 31])\n",
      "`output: torch.Size([1, 31, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 32])\n",
      "tgt_pos: torch.Size([1, 32])\n",
      "tgt: torch.Size([1, 32, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([32, 32])\n",
      "`output: torch.Size([1, 32, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 33])\n",
      "tgt_pos: torch.Size([1, 33])\n",
      "tgt: torch.Size([1, 33, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([33, 33])\n",
      "`output: torch.Size([1, 33, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 34])\n",
      "tgt_pos: torch.Size([1, 34])\n",
      "tgt: torch.Size([1, 34, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([34, 34])\n",
      "`output: torch.Size([1, 34, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 35])\n",
      "tgt_pos: torch.Size([1, 35])\n",
      "tgt: torch.Size([1, 35, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([35, 35])\n",
      "`output: torch.Size([1, 35, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 36])\n",
      "tgt_pos: torch.Size([1, 36])\n",
      "tgt: torch.Size([1, 36, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([36, 36])\n",
      "`output: torch.Size([1, 36, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 37])\n",
      "tgt_pos: torch.Size([1, 37])\n",
      "tgt: torch.Size([1, 37, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([37, 37])\n",
      "`output: torch.Size([1, 37, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 38])\n",
      "tgt_pos: torch.Size([1, 38])\n",
      "tgt: torch.Size([1, 38, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([38, 38])\n",
      "`output: torch.Size([1, 38, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 39])\n",
      "tgt_pos: torch.Size([1, 39])\n",
      "tgt: torch.Size([1, 39, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([39, 39])\n",
      "`output: torch.Size([1, 39, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 40])\n",
      "tgt_pos: torch.Size([1, 40])\n",
      "tgt: torch.Size([1, 40, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([40, 40])\n",
      "`output: torch.Size([1, 40, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 41])\n",
      "tgt_pos: torch.Size([1, 41])\n",
      "tgt: torch.Size([1, 41, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([41, 41])\n",
      "`output: torch.Size([1, 41, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 42])\n",
      "tgt_pos: torch.Size([1, 42])\n",
      "tgt: torch.Size([1, 42, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([42, 42])\n",
      "`output: torch.Size([1, 42, 30522])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 43])\n",
      "tgt_pos: torch.Size([1, 43])\n",
      "tgt: torch.Size([1, 43, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([43, 43])\n",
      "`output: torch.Size([1, 43, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 44])\n",
      "tgt_pos: torch.Size([1, 44])\n",
      "tgt: torch.Size([1, 44, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([44, 44])\n",
      "`output: torch.Size([1, 44, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 45])\n",
      "tgt_pos: torch.Size([1, 45])\n",
      "tgt: torch.Size([1, 45, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([45, 45])\n",
      "`output: torch.Size([1, 45, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 46])\n",
      "tgt_pos: torch.Size([1, 46])\n",
      "tgt: torch.Size([1, 46, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([46, 46])\n",
      "`output: torch.Size([1, 46, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 47])\n",
      "tgt_pos: torch.Size([1, 47])\n",
      "tgt: torch.Size([1, 47, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([47, 47])\n",
      "`output: torch.Size([1, 47, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 48])\n",
      "tgt_pos: torch.Size([1, 48])\n",
      "tgt: torch.Size([1, 48, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([48, 48])\n",
      "`output: torch.Size([1, 48, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 49])\n",
      "tgt_pos: torch.Size([1, 49])\n",
      "tgt: torch.Size([1, 49, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([49, 49])\n",
      "`output: torch.Size([1, 49, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 50])\n",
      "tgt_pos: torch.Size([1, 50])\n",
      "tgt: torch.Size([1, 50, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([50, 50])\n",
      "`output: torch.Size([1, 50, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 51])\n",
      "tgt_pos: torch.Size([1, 51])\n",
      "tgt: torch.Size([1, 51, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([51, 51])\n",
      "`output: torch.Size([1, 51, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 52])\n",
      "tgt_pos: torch.Size([1, 52])\n",
      "tgt: torch.Size([1, 52, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([52, 52])\n",
      "`output: torch.Size([1, 52, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 53])\n",
      "tgt_pos: torch.Size([1, 53])\n",
      "tgt: torch.Size([1, 53, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([53, 53])\n",
      "`output: torch.Size([1, 53, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 54])\n",
      "tgt_pos: torch.Size([1, 54])\n",
      "tgt: torch.Size([1, 54, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([54, 54])\n",
      "`output: torch.Size([1, 54, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 55])\n",
      "tgt_pos: torch.Size([1, 55])\n",
      "tgt: torch.Size([1, 55, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([55, 55])\n",
      "`output: torch.Size([1, 55, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 56])\n",
      "tgt_pos: torch.Size([1, 56])\n",
      "tgt: torch.Size([1, 56, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([56, 56])\n",
      "`output: torch.Size([1, 56, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 57])\n",
      "tgt_pos: torch.Size([1, 57])\n",
      "tgt: torch.Size([1, 57, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([57, 57])\n",
      "`output: torch.Size([1, 57, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 58])\n",
      "tgt_pos: torch.Size([1, 58])\n",
      "tgt: torch.Size([1, 58, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([58, 58])\n",
      "`output: torch.Size([1, 58, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 59])\n",
      "tgt_pos: torch.Size([1, 59])\n",
      "tgt: torch.Size([1, 59, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([59, 59])\n",
      "`output: torch.Size([1, 59, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 60])\n",
      "tgt_pos: torch.Size([1, 60])\n",
      "tgt: torch.Size([1, 60, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([60, 60])\n",
      "`output: torch.Size([1, 60, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 61])\n",
      "tgt_pos: torch.Size([1, 61])\n",
      "tgt: torch.Size([1, 61, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([61, 61])\n",
      "`output: torch.Size([1, 61, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 62])\n",
      "tgt_pos: torch.Size([1, 62])\n",
      "tgt: torch.Size([1, 62, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([62, 62])\n",
      "`output: torch.Size([1, 62, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 63])\n",
      "tgt_pos: torch.Size([1, 63])\n",
      "tgt: torch.Size([1, 63, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([63, 63])\n",
      "`output: torch.Size([1, 63, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 64])\n",
      "tgt_pos: torch.Size([1, 64])\n",
      "tgt: torch.Size([1, 64, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([64, 64])\n",
      "`output: torch.Size([1, 64, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 65])\n",
      "tgt_pos: torch.Size([1, 65])\n",
      "tgt: torch.Size([1, 65, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([65, 65])\n",
      "`output: torch.Size([1, 65, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 66])\n",
      "tgt_pos: torch.Size([1, 66])\n",
      "tgt: torch.Size([1, 66, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([66, 66])\n",
      "`output: torch.Size([1, 66, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 67])\n",
      "tgt_pos: torch.Size([1, 67])\n",
      "tgt: torch.Size([1, 67, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([67, 67])\n",
      "`output: torch.Size([1, 67, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 68])\n",
      "tgt_pos: torch.Size([1, 68])\n",
      "tgt: torch.Size([1, 68, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([68, 68])\n",
      "`output: torch.Size([1, 68, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 69])\n",
      "tgt_pos: torch.Size([1, 69])\n",
      "tgt: torch.Size([1, 69, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([69, 69])\n",
      "`output: torch.Size([1, 69, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 70])\n",
      "tgt_pos: torch.Size([1, 70])\n",
      "tgt: torch.Size([1, 70, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([70, 70])\n",
      "`output: torch.Size([1, 70, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 71])\n",
      "tgt_pos: torch.Size([1, 71])\n",
      "tgt: torch.Size([1, 71, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([71, 71])\n",
      "`output: torch.Size([1, 71, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 72])\n",
      "tgt_pos: torch.Size([1, 72])\n",
      "tgt: torch.Size([1, 72, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([72, 72])\n",
      "`output: torch.Size([1, 72, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 73])\n",
      "tgt_pos: torch.Size([1, 73])\n",
      "tgt: torch.Size([1, 73, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([73, 73])\n",
      "`output: torch.Size([1, 73, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 74])\n",
      "tgt_pos: torch.Size([1, 74])\n",
      "tgt: torch.Size([1, 74, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([74, 74])\n",
      "`output: torch.Size([1, 74, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 75])\n",
      "tgt_pos: torch.Size([1, 75])\n",
      "tgt: torch.Size([1, 75, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([75, 75])\n",
      "`output: torch.Size([1, 75, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 76])\n",
      "tgt_pos: torch.Size([1, 76])\n",
      "tgt: torch.Size([1, 76, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([76, 76])\n",
      "`output: torch.Size([1, 76, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 77])\n",
      "tgt_pos: torch.Size([1, 77])\n",
      "tgt: torch.Size([1, 77, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([77, 77])\n",
      "`output: torch.Size([1, 77, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 78])\n",
      "tgt_pos: torch.Size([1, 78])\n",
      "tgt: torch.Size([1, 78, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([78, 78])\n",
      "`output: torch.Size([1, 78, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 79])\n",
      "tgt_pos: torch.Size([1, 79])\n",
      "tgt: torch.Size([1, 79, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([79, 79])\n",
      "`output: torch.Size([1, 79, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 80])\n",
      "tgt_pos: torch.Size([1, 80])\n",
      "tgt: torch.Size([1, 80, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([80, 80])\n",
      "`output: torch.Size([1, 80, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 81])\n",
      "tgt_pos: torch.Size([1, 81])\n",
      "tgt: torch.Size([1, 81, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([81, 81])\n",
      "`output: torch.Size([1, 81, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 82])\n",
      "tgt_pos: torch.Size([1, 82])\n",
      "tgt: torch.Size([1, 82, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([82, 82])\n",
      "`output: torch.Size([1, 82, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 83])\n",
      "tgt_pos: torch.Size([1, 83])\n",
      "tgt: torch.Size([1, 83, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([83, 83])\n",
      "`output: torch.Size([1, 83, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 84])\n",
      "tgt_pos: torch.Size([1, 84])\n",
      "tgt: torch.Size([1, 84, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([84, 84])\n",
      "`output: torch.Size([1, 84, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 85])\n",
      "tgt_pos: torch.Size([1, 85])\n",
      "tgt: torch.Size([1, 85, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([85, 85])\n",
      "`output: torch.Size([1, 85, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 86])\n",
      "tgt_pos: torch.Size([1, 86])\n",
      "tgt: torch.Size([1, 86, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([86, 86])\n",
      "`output: torch.Size([1, 86, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 87])\n",
      "tgt_pos: torch.Size([1, 87])\n",
      "tgt: torch.Size([1, 87, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([87, 87])\n",
      "`output: torch.Size([1, 87, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 88])\n",
      "tgt_pos: torch.Size([1, 88])\n",
      "tgt: torch.Size([1, 88, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([88, 88])\n",
      "`output: torch.Size([1, 88, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 89])\n",
      "tgt_pos: torch.Size([1, 89])\n",
      "tgt: torch.Size([1, 89, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([89, 89])\n",
      "`output: torch.Size([1, 89, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 90])\n",
      "tgt_pos: torch.Size([1, 90])\n",
      "tgt: torch.Size([1, 90, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([90, 90])\n",
      "`output: torch.Size([1, 90, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 91])\n",
      "tgt_pos: torch.Size([1, 91])\n",
      "tgt: torch.Size([1, 91, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([91, 91])\n",
      "`output: torch.Size([1, 91, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 92])\n",
      "tgt_pos: torch.Size([1, 92])\n",
      "tgt: torch.Size([1, 92, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([92, 92])\n",
      "`output: torch.Size([1, 92, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 93])\n",
      "tgt_pos: torch.Size([1, 93])\n",
      "tgt: torch.Size([1, 93, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([93, 93])\n",
      "`output: torch.Size([1, 93, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 94])\n",
      "tgt_pos: torch.Size([1, 94])\n",
      "tgt: torch.Size([1, 94, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([94, 94])\n",
      "`output: torch.Size([1, 94, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 95])\n",
      "tgt_pos: torch.Size([1, 95])\n",
      "tgt: torch.Size([1, 95, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([95, 95])\n",
      "`output: torch.Size([1, 95, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 96])\n",
      "tgt_pos: torch.Size([1, 96])\n",
      "tgt: torch.Size([1, 96, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([96, 96])\n",
      "`output: torch.Size([1, 96, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 97])\n",
      "tgt_pos: torch.Size([1, 97])\n",
      "tgt: torch.Size([1, 97, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([97, 97])\n",
      "`output: torch.Size([1, 97, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 98])\n",
      "tgt_pos: torch.Size([1, 98])\n",
      "tgt: torch.Size([1, 98, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([98, 98])\n",
      "`output: torch.Size([1, 98, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 99])\n",
      "tgt_pos: torch.Size([1, 99])\n",
      "tgt: torch.Size([1, 99, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([99, 99])\n",
      "`output: torch.Size([1, 99, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "tgt: torch.Size([1, 100])\n",
      "tgt_pos: torch.Size([1, 100])\n",
      "tgt: torch.Size([1, 100, 768])\n",
      "memory: torch.Size([1, 5, 768])\n",
      "tgt_mask: torch.Size([100, 100])\n",
      "`output: torch.Size([1, 100, 30522])\n",
      "predicted_id: 1520\n",
      "gen_word: ‘\n",
      "Prompt: Australia is so\n",
      "Generated Text: [PAD] ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘ ‘\n"
     ]
    }
   ],
   "source": [
    "def generate_text_attention(model, seed_text, max_length):\n",
    "    model.eval()\n",
    "    #seed_text: string\n",
    "    input_ids = tokenizer.encode(seed_text, add_special_tokens=True, truncation=True)\n",
    "    # explain args\n",
    "    \n",
    "    input_tensor = torch.tensor(input_ids).unsqueeze(1).to(DEVICE)  # Add batch dimension\n",
    "    # input_sensor: (batch_size, max_length)\n",
    "    generated_id_sequence = []\n",
    "    #why does transformer use SOS token, but attention doesn't?\n",
    "    for i in range(max_length):\n",
    "        torch.no_grad()\n",
    "        #input_tensor: (batch_size, min(max_length, len(input_ids)+i+1))\n",
    "        output = model(input_tensor)\n",
    "        #output: torch.Size([max_length, 1, vocab_size])\n",
    "        # Take the most likely word from the last output position\n",
    "        predicted_id = output[-1, :, :].argmax(dim=1).item()\n",
    "        generated_id_sequence.append(predicted_id)\n",
    "        gen_word = tokenizer.decode([predicted_id])\n",
    "        #print(f\"gen_word: {gen_word}\")\n",
    "        input_tensor = torch.cat([input_tensor,torch.tensor([[predicted_id]]).to(DEVICE)], dim=0)\n",
    "        \n",
    "        if input_tensor.shape[0] >= max_length: #ensure input not larger than context window\n",
    "            input_tensor = input_tensor[1:]\n",
    "        \n",
    "        print(tokenizer.decode(input_ids + generated_id_sequence))\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        if gen_word == \".\" or predicted_id == tokenizer.sep_token_id:\n",
    "            break\n",
    "    gen_sequence = tokenizer.decode(generated_id_sequence)\n",
    "    print(f\"Prompt: {seed_text}\")\n",
    "    print(f\"Generated Text: {gen_sequence}\")\n",
    "    \n",
    "def generate_text_transformer(model, seed_text, max_length):\n",
    "    model.eval()\n",
    "    #seed_text: string\n",
    "    #input_ids = tokenizer.encode(seed_text, add_special_tokens=True, max_length=max_length, padding='max_length',truncation=True)\n",
    "    input_ids = tokenizer.encode(seed_text, add_special_tokens=True,truncation=True)\n",
    "    # explain\n",
    "    \n",
    "    #Input into the encoder\n",
    "    src_tensor = torch.tensor(input_ids).unsqueeze(0).to(DEVICE)  # Add batch dimension\n",
    "    # input_sensor: (batch_size, max_length)\n",
    "    \n",
    "    #Appending last decoder output (in token id form) each iteration \n",
    "    tgt_id_sequence = [0]\n",
    "    \n",
    "    src_mask = (torch.zeros(src_tensor.size(1), src_tensor.size(1))).type(torch.bool).to(DEVICE)\n",
    "    #what is the purpose of the src_mask?\n",
    "    print(f\"src_tensor: {src_tensor.shape}\")\n",
    "    memory = model.encode(src_tensor, src_mask)\n",
    "    print(f\"memory: {memory.shape}\")\n",
    "    for i in range(max_length):\n",
    "        torch.no_grad()\n",
    "        #Input into the decoder\n",
    "        tgt = torch.tensor(tgt_id_sequence).unsqueeze(0).to(DEVICE)\n",
    "        print(f\"tgt: {tgt.shape}\")\n",
    "        #tgt_mask = (model.generate_square_subsequent_mask(tgt.size(0)).type(torch.bool)).to(DEVICE)\n",
    "        #batch, seq\n",
    "        tgt_mask = (torch.zeros(tgt.size(1), tgt.size(1))).type(torch.bool).to(DEVICE)\n",
    "        output = model.decode(tgt, memory, tgt_mask)\n",
    "        print(f\"`output: {output.shape}\")\n",
    "        # Take the most likely word from the last position\n",
    "        predicted_id = output[:, -1, :].argmax(dim=1).item()\n",
    "        print(f\"predicted_id: {predicted_id}\")\n",
    "        tgt_id_sequence.append(predicted_id)\n",
    "        gen_word = tokenizer.decode([predicted_id])\n",
    "        print(f\"gen_word: {gen_word}\")\n",
    "\n",
    "        if gen_word == \".\" or predicted_id == tokenizer.sep_token_id:\n",
    "            break\n",
    "\n",
    "    gen_sequence = tokenizer.decode(tgt_id_sequence)\n",
    "    print(f\"Prompt: {seed_text}\")\n",
    "    print(f\"Generated Text: {gen_sequence}\")\n",
    "\n",
    "# Example of generating text\n",
    "#generate_text(model, train_data[40:40+max_length], max_length)\n",
    "#generate_text_attention(model, \"Australia is so\", max_length)\n",
    "generate_text_transformer(model, \"Australia is so\", max_length)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
